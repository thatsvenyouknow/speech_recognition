{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech_to_Text notebook to try out models and build a class that eventually automates testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load paths of recordings to pass to speech_to_text class and create dict with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "for file in natsorted(os.listdir(\"H:/Speech_to_Text/7\")):\n",
    "    paths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentences (partially generated with the General Purpose Service Robot Command Generator)\n",
    "tests = {1: \"pick up the bag\",\n",
    "         2: \"open the door\",\n",
    "         3: \"touch the desk\",\n",
    "         4: \"could you please hand the coke to lisa\",\n",
    "         5: \"put the spoon next to the bowl\",\n",
    "         6: \"place the tab inside the dishwasher\",\n",
    "         7: \"get the soap from the bar navigate to the sink and bring it to jacob\",\n",
    "         8: \" tell the time find the waving person in the living room, and escort him to the dining table\",\n",
    "         9: \"could you locate alexander in the bed guide him to the apartment, and follow emma who is at the tv stand\",\n",
    "         10: \"could you please take the left-most object from the bookcase to the shelf\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init class speech_to_text \n",
    "(When finished) handles the initialization and running of the different models, as well as comparison to ground truth and calculation of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class speech_to_text():\n",
    "\n",
    "    def __init__(self, models, ground_truth):\n",
    "        from tqdm.notebook import tqdm\n",
    "        self.models = models\n",
    "        self.detected_words = {}\n",
    "        self.ground_truth = ground_truth\n",
    "\n",
    "    def load_data(self, file_directory, fs = 44100):\n",
    "        from natsort import natsorted\n",
    "        import os\n",
    "        self.file_directory = file_directory\n",
    "        self.file_paths = [file for file in natsorted(os.listdir(file_directory))]\n",
    "        self.fs = fs\n",
    "\n",
    "\n",
    "    #VOSK block\n",
    "    def init_vosk(self, model_path = \"vosk-model-en-us-0.42-gigaspeech\"):\n",
    "        import json\n",
    "        from vosk import Model, KaldiRecognizer\n",
    "        assert os.path.exists(model_path), \"model not in current path\"\n",
    "        model = Model(model_path)\n",
    "        self.models = KaldiRecognizer(model, sample_rate)\n",
    "        self.models.SetWords(True)\n",
    "\n",
    "    def run_vosk(self, n_frames = 4000, thresh = 0):\n",
    "        assert type(self.models) == vosk.KaldiRecognizer, \"First initialize vosk model using 'init_vosk'\"\n",
    "        from scipy.io import wavfile\n",
    "        words = {k: [] for k in range(len(self.file_paths))}\n",
    "        confs = words.copy()\n",
    "\n",
    "        for ix, file in tqdm(enumerate(self.file_paths)):\n",
    "            _, audio = wavfile.read(\"{}/{}\".format(self.file_directory, file))\n",
    "            audio_bytes = bytes(bytearray(audio))\n",
    "\n",
    "            #Run Speech Recognizer\n",
    "            i = 0\n",
    "            while True:\n",
    "                data = audio_bytes[n_frames*i:n_frames*(i+1)]\n",
    "                i += 1\n",
    "\n",
    "                if data == b\"\": #if indexing is out of range\n",
    "                    break\n",
    "\n",
    "                if self.models.AcceptWaveform(data):\n",
    "                    try: #if section does not contain a word, we get KeyError\n",
    "                        instance = json.loads(rec.Result())\n",
    "                        #if min. confidence, append detected words to lists\n",
    "                        for entry in instance[\"result\"]:\n",
    "                            if entry[\"conf\"] > thresh:\n",
    "                                confs[ix].append(entry[\"conf\"])\n",
    "                                words[ix].append(entry[\"word\"])\n",
    "\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "            #last detected word is not in rec.Result(), therefore...\n",
    "            last_instance = json.loads(rec.FinalResult())\n",
    "            for entry in last_instance[\"result\"]:\n",
    "                if entry[\"conf\"] > thresh:\n",
    "                    confs[ix].append(entry[\"conf\"])\n",
    "                    words[ix].append(entry[\"word\"])\n",
    "\n",
    "\n",
    "\n",
    "    #Whisper block\n",
    "    def init_whisper(self, mode = \"base\"):\n",
    "        \"\"\"\n",
    "        mode: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "        \"\"\"\n",
    "        import whisper\n",
    "        #CUDA_LAUNCH_BLOCKING=1\n",
    "        model = whisper.load_model(mode)\n",
    "\n",
    "    def run_whisper(self):\n",
    "        \"\"\"\n",
    "        Note: Internally, the transcribe() method reads the entire file and processes the audio \n",
    "            with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions \n",
    "            on each window.\n",
    "        \"\"\"\n",
    "        words = {k: [] for k in range(len(self.file_paths))}\n",
    "        for ix, file in tqdm(enumerate(self.file_paths)):\n",
    "            audio = whisper.load_audio(file)\n",
    "            audio = whisper.pad_or_trim(audio) #pad/trim to fit 30 seconds\n",
    "            words[ix]  = model.transcribe(audio)[\"text\"]\n",
    "    \n",
    "\n",
    "    #Google Cloud Speech API block\n",
    "    def init_google(self, model = \"default\", key = \"google_key.json\"):\n",
    "        \"\"\"\n",
    "        model: \"default\", \"video\", \"command_and_search\"\n",
    "        key: path to Google Service key [json]\n",
    "        Note: Cloud Speech-to-Text API needs to be enabled in your account!\n",
    "        \"\"\"\n",
    "        from google.cloud import speech\n",
    "        assert os.path.exists(key), \"path to key does not exist\"\n",
    "        client = speech.SpeechClient.from_service_account_file(key)\n",
    "\n",
    "        #Note: for non-wav (or non-flac) files, an encoding parameter must be passed to config\n",
    "        config = speech.RecognitionConfig(\n",
    "            sample_rate_hertz = 44100,\n",
    "            language_code = \"en-US\",\n",
    "            #model = default\n",
    "        )\n",
    "\n",
    "    def run_google():\n",
    "        words = {k: [] for k in range(len(self.file_paths))}\n",
    "        for ix, file in tqdm(enumerate(self.file_paths)):\n",
    "            with open(file, \"rb\") as f:\n",
    "                audio = f.read()\n",
    "                audio_file = speech.RecognitionAudio(content = audio)\n",
    "                response = client.recognize(\n",
    "                    config = config,\n",
    "                    audio = audio_file\n",
    "                )\n",
    "                words[ix] = response.results[0].alternatives[0].transcript        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Playground (before putting them into the class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import wave\n",
    "obj = wave.open(\"H:/Speech_to_Text/7/2_7.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vosk\n",
    "import vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fs, audio = wavfile.read(\"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vosk (based on Kaldi toolbox)\n",
    "Note: there are 2 different models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Make sure the \"model\" folder is in current path\n",
    "#model = \"GigaSpeech_ASR_XL\"\n",
    "model = \"vosk-model-en-us-0.42-gigaspeech\"\n",
    "\n",
    "os.chdir(\"C:/Users/Daydreamore/Desktop/pythonProject/Speech_to_Text\")\n",
    "assert os.path.exists(model), \"model not in current path\"\n",
    "\n",
    "#Initialize model\n",
    "model = Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "rec = KaldiRecognizer(model, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c238a0686ff4a30892625c56dd2d743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize KaldiRecognizer\n",
    "sample_rate = 44100\n",
    "audio_bytes = bytes(bytearray(audio))\n",
    "n_frames = 4000\n",
    "thresh = 0\n",
    "conf = [] #store confidence of detected word\n",
    "word = [] #store words\n",
    "\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "rec.SetWords(True)\n",
    "\n",
    "#Initialize a time bar to see progress when analyzing\n",
    "file_size = len(audio)\n",
    "pbar = tqdm(total=file_size)\n",
    "\n",
    "#Run Speech Recognizer\n",
    "i = 0\n",
    "while True:\n",
    "    data = audio_bytes[n_frames*i:n_frames*(i+1)]\n",
    "    i += 1\n",
    "    pbar.update(len(data)/2) #updates progress bar\n",
    "\n",
    "    if data == b\"\": #if indexing is out of range, we get b\"\" as output\n",
    "        break\n",
    "\n",
    "    if rec.AcceptWaveform(data):\n",
    "        try: #if section does not contain a word, we get KeyError\n",
    "            instance = json.loads(rec.Result())\n",
    "            #if min. confidence, append detected words to lists\n",
    "            for entry in instance[\"result\"]:\n",
    "                if entry[\"conf\"] > thresh:\n",
    "                    conf.append(entry[\"conf\"])\n",
    "                    word.append(entry[\"word\"])\n",
    "\n",
    "        except KeyError: #catch error\n",
    "            continue\n",
    "\n",
    "#last detected word is not in rec.Result(), therefore...\n",
    "last_instance = json.loads(rec.FinalResult())\n",
    "for entry in last_instance[\"result\"]:\n",
    "    if entry[\"conf\"] > thresh:\n",
    "        conf.append(entry[\"conf\"])\n",
    "        word.append(entry[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get',\n",
       " 'the',\n",
       " 'soap',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bar',\n",
       " 'navigate',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sink',\n",
       " 'and',\n",
       " 'bring',\n",
       " 'it',\n",
       " 'to',\n",
       " 'jacob']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Whisper (Open AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#wav_audio = \"C:/Users/Daydreamore/Desktop/Semester/Speech_to_Text/1/2_1.wav\"\n",
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "model = whisper.load_model(\"base\")\n",
    "audio = whisper.load_audio(\"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\") #\"C:/Users/Daydreamore/Desktop/record.mp3\"\n",
    "audio = whisper.pad_or_trim(audio) #pad/trim to fit 30 seconds\n",
    "#audio = audio.astype(dtype=\"float32\")\n",
    "result = model.transcribe(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Get the soap from the bar navigate to the sink and bring it to Jacob'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Speech to Text API ($)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio data can either be loaded from Google Cloud Storage or directly if the audio length is <1 minute and <10mb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the soap from the bar navigate to the sink and bring it to Jacob\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade google-cloud-speech\n",
    "#Documentation: https://cloud.google.com/speech-to-text/docs/speech-to-text-requests\n",
    "from google.cloud import speech\n",
    "client = speech.SpeechClient.from_service_account_file(\"google_key.json\")\n",
    "\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/7_7.wav\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    audio_data = f.read()\n",
    "\n",
    "audio_file = speech.RecognitionAudio(content = audio_data)\n",
    "\n",
    "#Note: for non-wav (or flac) files, an encoding parameter must be passed to config\n",
    "config = speech.RecognitionConfig(\n",
    "    sample_rate_hertz = 44100,\n",
    "    language_code = \"en-US\",\n",
    "    #model = default\n",
    ")\n",
    "\n",
    "response = client.recognize(\n",
    "    config = config,\n",
    "    audio = audio_file,\n",
    ")\n",
    "\n",
    "print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d5ec8849436b7a03feff4b060e46c6b60d2401a18d2e1ee71f98cd3f572d4d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
