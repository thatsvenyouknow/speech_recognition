{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech_to_Text notebook that displays short examples of the different models before defining the class that is used to initialize the models and run the comparison."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import general libraries for sorting, computing, interacting with OS\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#Import the speech-to-text models\n",
    "from google_api import init_google, run_google\n",
    "from vosk_api import init_vosk, run_vosk\n",
    "from wav2vec2_api import init_wav2vec2, run_wav2vec2\n",
    "from whisper_api import init_whisper, run_whisper\n",
    "\n",
    "#Import metrics\n",
    "from metrics import rtf, wer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short code snippets that allow running each model separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_api import init_google, run_google\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_google()\n",
    "out, time = run_google(file_path, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_api import init_whisper, run_whisper\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_whisper()\n",
    "out, time = run_whisper(file_path, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk_api import init_vosk, run_vosk\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_vosk()\n",
    "out, time = run_vosk(file_path, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\transformers\\feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('get the soap from the bar navigate to the sink and bring it to jacob',\n",
       " 1.696542501449585)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wav2vec2_api import init_wav2vec2, run_wav2vec2\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_wav2vec2()\n",
    "out, time = run_wav2vec2(file_path, setup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth for the recorded sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentences (partially generated with the General Purpose Service Robot Command Generator)\n",
    "tests = {1: \"pick up the bag\",\n",
    "         2: \"open the door\",\n",
    "         3: \"touch the desk\",\n",
    "         4: \"could you please hand the coke to lisa\",\n",
    "         5: \"put the spoon next to the bowl\",\n",
    "         6: \"place the tab inside the dishwasher\",\n",
    "         7: \"get the soap from the bar navigate to the sink and bring it to jacob\",\n",
    "         8: \"tell the time find the waving person in the living room, and escort him to the dining table\",\n",
    "         9: \"could you locate alexander in the bed guide him to the apartment, and follow emma who is at the tv stand\",\n",
    "         10: \"could you please take the leftmost object from the bookcase to the shelf\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Class \"speech_to_text\"\n",
    "Handles the initialization and running of the different models, as well as comparison to ground truth and calculation of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class speech_to_text():\n",
    "\n",
    "    def __init__(self):\n",
    "        #Initialize models that will be compared\n",
    "        self.setup_whisper = init_whisper()\n",
    "        self.setup_google = init_google()\n",
    "        self.setup_vosk = init_vosk()\n",
    "        self.setup_wav2vec2 = init_wav2vec2()\n",
    "        print(\"All models initialized\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def load_data(self, file_directory, ground_truth):\n",
    "        \"\"\"\n",
    "        - file_directory: directory that contains audio files (.wav)\n",
    "        - ground truth: true text of the speech in the audio files\n",
    "        \"\"\"\n",
    "        self.file_directory = file_directory\n",
    "        self.ground_truth = ground_truth\n",
    "        self.file_paths = [file for file in natsorted(os.listdir(file_directory))]\n",
    "\n",
    "    def compare(self):\n",
    "        \"\"\"\n",
    "        Compares the different models' outputs in terms of\n",
    "        Word Error Rate (WER) and Real-Time Factor (RTF)\n",
    "        \"\"\" \n",
    "        out_dict = {}\n",
    "        sum_dict = {\"Whisper\": {\"WER\": [], \"RTF\": []},\n",
    "                    \"Google\": {\"WER\": [], \"RTF\": []},\n",
    "                    \"Vosk\": {\"WER\": [], \"RTF\": []},\n",
    "                    \"Wav2vec2\": {\"WER\": [], \"RTF\": []}\n",
    "                    }\n",
    "        for ix, file in enumerate(self.file_paths):\n",
    "            file_name = \"{}/{}\".format(self.file_directory, file)\n",
    "\n",
    "            #Run models\n",
    "            out_whisper, time_whisper = run_whisper(file_name, self.setup_whisper)\n",
    "            out_google, time_google = run_google(file_name, self.setup_google)\n",
    "            out_vosk, time_vosk = run_vosk(file_name, self.setup_vosk)\n",
    "            out_wav2vec2, time_wav2vec2 = run_wav2vec2(file_name, self.setup_wav2vec2)\n",
    "\n",
    "            #Store metrics & model output in a nested dictionary\n",
    "            test_num = str(ix+1)\n",
    "            out_dict[test_num] = {\n",
    "                \"Whisper\": {\"WER\": wer(self.ground_truth, out_whisper), \n",
    "                            \"RTF\": rtf(time_whisper, file_name), \n",
    "                            \"Model Output\": out_whisper},\n",
    "                \"Google\": {\"WER\": wer(self.ground_truth, out_google), \n",
    "                           \"RTF\": rtf(time_google, file_name),\n",
    "                           \"Model Output\": out_google},\n",
    "                \"Vosk\": {\"WER\": wer(self.ground_truth, out_vosk),\n",
    "                         \"RTF\": rtf(time_vosk, file_name), \n",
    "                         \"Model Output\": out_vosk},\n",
    "                \"Wav2vec2\": {\"WER\": wer(self.ground_truth, out_wav2vec2), \n",
    "                             \"RTF\": rtf(time_wav2vec2, file_name),\n",
    "                             \"Model Output\": out_wav2vec2}\n",
    "                }\n",
    "            \n",
    "            #Keep track of the metrics for each model to later summarize\n",
    "            for model in sum_dict.keys():\n",
    "                for metric in sum_dict[model].keys():\n",
    "                    sum_dict[model][metric].append(out_dict[test_num][model][metric])\n",
    "        \n",
    "        for model in sum_dict.keys():\n",
    "                for metric in sum_dict[model].keys():\n",
    "                    #Calculate mean for each metric for each model\n",
    "                    sum_dict[model][metric] = round(sum(sum_dict[model][metric])/len(sum_dict[model][metric]),2)\n",
    "        \n",
    "        #Save output (better than returning when running all models with all audios in a loop)\n",
    "        np.save(\"comparisons/{}_detailed.npy\".format(str(self.counter)),out_dict)\n",
    "        np.save(\"comparisons/{}_summarized.npy\".format(str(self.counter)),sum_dict)\n",
    "        self.counter += 1\n",
    "\n",
    "        #in case immediate output is desired\n",
    "        #return out_dict, sum_dict "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models initialized\n"
     ]
    }
   ],
   "source": [
    "#Initialize class\n",
    "test_class = speech_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Iterates over recording folder, loads data, and runs comparison\n",
    "for i in np.arange(1,11,1):\n",
    "    dir = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/{}\".format(str(i))\n",
    "    test_class.load_data(file_directory=dir, ground_truth=tests[i])\n",
    "    test_class.compare()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After comparison: Load saved data of comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pick up the bag\n",
      "{'Whisper': {'WER': 0.82, 'RTF': 0.76}, 'Google': {'WER': 1.34, 'RTF': 0.67}, 'Vosk': {'WER': 1.38, 'RTF': 1.25}, 'Wav2vec2': {'WER': 1.46, 'RTF': 0.48}}\n",
      "\n",
      "\n",
      "open the door\n",
      "{'Whisper': {'WER': 0.33, 'RTF': 0.6}, 'Google': {'WER': 1.0, 'RTF': 0.43}, 'Vosk': {'WER': 1.36, 'RTF': 0.92}, 'Wav2vec2': {'WER': 1.14, 'RTF': 0.37}}\n",
      "\n",
      "\n",
      "touch the desk\n",
      "{'Whisper': {'WER': 5.79, 'RTF': 1.54}, 'Google': {'WER': 2.46, 'RTF': 0.75}, 'Vosk': {'WER': 2.15, 'RTF': 1.64}, 'Wav2vec2': {'WER': 2.05, 'RTF': 0.38}}\n",
      "\n",
      "\n",
      "could you please hand the coke to lisa\n",
      "{'Whisper': {'WER': 1.06, 'RTF': 0.4}, 'Google': {'WER': 1.7, 'RTF': 0.56}, 'Vosk': {'WER': 1.91, 'RTF': 1.17}, 'Wav2vec2': {'WER': 1.4, 'RTF': 0.32}}\n",
      "\n",
      "\n",
      "put the spoon next to the bowl\n",
      "{'Whisper': {'WER': 0.9, 'RTF': 0.38}, 'Google': {'WER': 1.74, 'RTF': 0.57}, 'Vosk': {'WER': 1.56, 'RTF': 1.01}, 'Wav2vec2': {'WER': 1.47, 'RTF': 0.33}}\n",
      "\n",
      "\n",
      "place the tab inside the dishwasher\n",
      "{'Whisper': {'WER': 0.69, 'RTF': 0.29}, 'Google': {'WER': 0.96, 'RTF': 0.43}, 'Vosk': {'WER': 1.52, 'RTF': 1.08}, 'Wav2vec2': {'WER': 1.13, 'RTF': 0.33}}\n",
      "\n",
      "\n",
      "get the soap from the bar navigate to the sink and bring it to jacob\n",
      "{'Whisper': {'WER': 0.8, 'RTF': 0.29}, 'Google': {'WER': 1.24, 'RTF': 0.35}, 'Vosk': {'WER': 1.52, 'RTF': 0.73}, 'Wav2vec2': {'WER': 0.78, 'RTF': 0.31}}\n",
      "\n",
      "\n",
      "tell the time find the waving person in the living room, and escort him to the dining table\n",
      "{'Whisper': {'WER': 0.37, 'RTF': 0.17}, 'Google': {'WER': 0.6, 'RTF': 0.32}, 'Vosk': {'WER': 1.17, 'RTF': 0.66}, 'Wav2vec2': {'WER': 0.35, 'RTF': 0.34}}\n",
      "\n",
      "\n",
      "could you locate alexander in the bed guide him to the apartment, and follow emma who is at the tv stand\n",
      "{'Whisper': {'WER': 0.68, 'RTF': 0.31}, 'Google': {'WER': 1.0, 'RTF': 0.39}, 'Vosk': {'WER': 1.59, 'RTF': 0.92}, 'Wav2vec2': {'WER': 0.73, 'RTF': 0.32}}\n",
      "\n",
      "\n",
      "could you please take the left-most object from the bookcase to the shelf\n",
      "{'Whisper': {'WER': 2.79, 'RTF': 0.26}, 'Google': {'WER': 3.58, 'RTF': 0.43}, 'Vosk': {'WER': 4.32, 'RTF': 1.14}, 'Wav2vec2': {'WER': 3.94, 'RTF': 0.97}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(10):\n",
    "    print(tests[i+1])\n",
    "    print(np.load(\"comparisons/{}_summarized.npy\".format(i), allow_pickle=True).item())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d5ec8849436b7a03feff4b060e46c6b60d2401a18d2e1ee71f98cd3f572d4d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
