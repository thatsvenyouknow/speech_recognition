{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech_to_Text notebook to try out models and build a class that eventually automates testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from vosk import Model, KaldiRecognizer\n",
    "\n",
    "\n",
    "from importlib import reload\n",
    "from scipy.io import wavfile\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load paths of recordings to pass to speech_to_text class and create dict with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "for file in natsorted(os.listdir(\"H:/Speech_to_Text/7\")):\n",
    "    paths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentences (partially generated with the General Purpose Service Robot Command Generator)\n",
    "tests = {1: \"pick up the bag\",\n",
    "         2: \"open the door\",\n",
    "         3: \"touch the desk\",\n",
    "         4: \"could you please hand the coke to lisa\",\n",
    "         5: \"put the spoon next to the bowl\",\n",
    "         6: \"place the tab inside the dishwasher\",\n",
    "         7: \"get the soap from the bar navigate to the sink and bring it to jacob\",\n",
    "         8: \" tell the time find the waving person in the living room, and escort him to the dining table\",\n",
    "         9: \"could you locate alexander in the bed guide him to the apartment, and follow emma who is at the tv stand\",\n",
    "         10: \"could you please take the left-most object from the bookcase to the shelf\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init class speech_to_text \n",
    "(When finished) handles the initialization and running of the different models, as well as comparison to ground truth and calculation of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class speech_to_text():\n",
    "\n",
    "    def __init__(self, models, ground_truth):\n",
    "        from tqdm.notebook import tqdm\n",
    "        self.models = models\n",
    "        self.detected_words = {}\n",
    "        self.ground_truth = ground_truth\n",
    "\n",
    "    def load_data(self, file_directory, fs = 44100):\n",
    "        from natsort import natsorted\n",
    "        import os\n",
    "        self.file_directory = file_directory\n",
    "        self.file_paths = [file for file in natsorted(os.listdir(file_directory))]\n",
    "        self.fs = fs\n",
    "\n",
    "\n",
    "    #VOSK block\n",
    "    def init_vosk(self, model_path = \"vosk-model-en-us-0.42-gigaspeech\"):\n",
    "        import json\n",
    "        from vosk import Model, KaldiRecognizer\n",
    "        assert os.path.exists(model_path), \"model not in current path\"\n",
    "        model = Model(model_path)\n",
    "        self.models = KaldiRecognizer(model, sample_rate)\n",
    "        self.models.SetWords(True)\n",
    "\n",
    "    def run_vosk(self, n_frames = 4000, thresh = 0):\n",
    "        assert type(self.models) == vosk.KaldiRecognizer, \"First initialize vosk model using 'init_vosk'\"\n",
    "        from scipy.io import wavfile\n",
    "        words = {k: [] for k in range(len(self.file_paths))}\n",
    "        confs = words.copy()\n",
    "\n",
    "        for ix, file in tqdm(enumerate(self.file_paths)):\n",
    "            _, audio = wavfile.read(\"{}/{}\".format(self.file_directory, file))\n",
    "            audio_bytes = bytes(bytearray(audio))\n",
    "\n",
    "            #Run Speech Recognizer\n",
    "            i = 0\n",
    "            while True:\n",
    "                data = audio_bytes[n_frames*i:n_frames*(i+1)]\n",
    "                i += 1\n",
    "\n",
    "                if data == b\"\": #if indexing is out of range\n",
    "                    break\n",
    "\n",
    "                if self.models.AcceptWaveform(data):\n",
    "                    try: #if section does not contain a word, we get KeyError\n",
    "                        instance = json.loads(rec.Result())\n",
    "                        #if min. confidence, append detected words to lists\n",
    "                        for entry in instance[\"result\"]:\n",
    "                            if entry[\"conf\"] > thresh:\n",
    "                                confs[ix].append(entry[\"conf\"])\n",
    "                                words[ix].append(entry[\"word\"])\n",
    "\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "            #last detected word is not in rec.Result(), therefore...\n",
    "            last_instance = json.loads(rec.FinalResult())\n",
    "            for entry in last_instance[\"result\"]:\n",
    "                if entry[\"conf\"] > thresh:\n",
    "                    confs[ix].append(entry[\"conf\"])\n",
    "                    words[ix].append(entry[\"word\"])\n",
    "\n",
    "\n",
    "\n",
    "    #Whisper block\n",
    "    def init_whisper(self, mode = \"base\"):\n",
    "        \"\"\"\n",
    "        mode: \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "        \"\"\"\n",
    "        import whisper\n",
    "        #CUDA_LAUNCH_BLOCKING=1\n",
    "        model = whisper.load_model(mode)\n",
    "\n",
    "    def run_whisper(self):\n",
    "        \"\"\"\n",
    "        Note: Internally, the transcribe() method reads the entire file and processes the audio \n",
    "            with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions \n",
    "            on each window.\n",
    "        \"\"\"\n",
    "        words = {k: [] for k in range(len(self.file_paths))}\n",
    "        for ix, file in tqdm(enumerate(self.file_paths)):\n",
    "            audio = whisper.load_audio(file)\n",
    "            audio = whisper.pad_or_trim(audio) #pad/trim to fit 30 seconds\n",
    "            words[ix]  = model.transcribe(audio)[\"text\"]\n",
    "    \n",
    "\n",
    "    #Google Cloud Speech API block\n",
    "    def init_google(self, model = \"default\", key = \"google_key.json\"):\n",
    "        \"\"\"\n",
    "        model: \"default\", \"video\", \"command_and_search\"\n",
    "        key: path to Google Service key [json]\n",
    "        Note: Cloud Speech-to-Text API needs to be enabled in your account!\n",
    "        \"\"\"\n",
    "        from google.cloud import speech\n",
    "        assert os.path.exists(key), \"path to key does not exist\"\n",
    "        client = speech.SpeechClient.from_service_account_file(key)\n",
    "\n",
    "        #Note: for non-wav (or non-flac) files, an encoding parameter must be passed to config\n",
    "        config = speech.RecognitionConfig(\n",
    "            sample_rate_hertz = 44100,\n",
    "            language_code = \"en-US\",\n",
    "            #model = default\n",
    "        )\n",
    "\n",
    "    def run_google():\n",
    "        words = {k: [] for k in range(len(self.file_paths))}\n",
    "        for ix, file in tqdm(enumerate(self.file_paths)):\n",
    "            with open(file, \"rb\") as f:\n",
    "                audio = f.read()\n",
    "                audio_file = speech.RecognitionAudio(content = audio)\n",
    "                response = client.recognize(\n",
    "                    config = config,\n",
    "                    audio = audio_file\n",
    "                )\n",
    "                words[ix] = response.results[0].alternatives[0].transcript   \n",
    "\n",
    "\n",
    "    #Vac2Vec 2.0 block\n",
    "    def init_vac2vec2(self, model_name = \"facebook/wav2vec2-large-960h-lv60-self\"):\n",
    "        \"\"\"\n",
    "        model: pretrained model to use (default: 1.18GB facebook model)\n",
    "        \"\"\"\n",
    "        from transformers import *\n",
    "        import torch\n",
    "        import soundfile as sf\n",
    "        import torchaudio\n",
    "\n",
    "        model_name = model_name \n",
    "        processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "    def run_vac2vec2(self):\n",
    "        for ix, file in tqdm(enumerate(self.file_paths)):\n",
    "            speech, sr = torchaudio.load(file)\n",
    "            speech = speech.squeeze()\n",
    "        \n",
    "            # resample sampling rate to 16000 (to work with pretrained model)\n",
    "            resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "            speech = resampler(speech)\n",
    "\n",
    "            # tokenize the wav\n",
    "            input_values = processor(speech, return_tensors=\"pt\", sampling_rate=16000)[\"input_values\"]\n",
    "\n",
    "            # perform inference\n",
    "            logits = model(input_values)[\"logits\"]\n",
    "\n",
    "            # use argmax to get the predicted IDs\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # decode the IDs to text and save in words container\n",
    "            transcription = processor.decode(predicted_ids[0])\n",
    "            words[ix] = transcription.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Playground (before putting them into the class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'H:/Speech_to_Text/7/2_7.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwave\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m obj \u001b[39m=\u001b[39m wave\u001b[39m.\u001b[39;49mopen(\u001b[39m\"\u001b[39;49m\u001b[39mH:/Speech_to_Text/7/2_7.wav\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\wave.py:509\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    507\u001b[0m         mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 509\u001b[0m     \u001b[39mreturn\u001b[39;00m Wave_read(f)\n\u001b[0;32m    510\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    511\u001b[0m     \u001b[39mreturn\u001b[39;00m Wave_write(f)\n",
      "File \u001b[1;32mc:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\wave.py:159\u001b[0m, in \u001b[0;36mWave_read.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i_opened_the_file \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 159\u001b[0m     f \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_i_opened_the_file \u001b[39m=\u001b[39m f\n\u001b[0;32m    161\u001b[0m \u001b[39m# else, assume it is an open file object already\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'H:/Speech_to_Text/7/2_7.wav'"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "obj = wave.open(\"H:/Speech_to_Text/7/2_7.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vosk\n",
    "import vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fs, audio = wavfile.read(\"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vosk (based on Kaldi toolbox)\n",
    "Note: there are 2 different models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Make sure the \"model\" folder is in current path\n",
    "#model = \"GigaSpeech_ASR_XL\"\n",
    "model = \"vosk-model-en-us-0.42-gigaspeech\"\n",
    "\n",
    "os.chdir(\"C:/Users/Daydreamore/Desktop/pythonProject/Speech_to_Text\")\n",
    "assert os.path.exists(model), \"model not in current path\"\n",
    "\n",
    "#Initialize model\n",
    "model = Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "rec = KaldiRecognizer(model, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c238a0686ff4a30892625c56dd2d743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize KaldiRecognizer\n",
    "sample_rate = 44100\n",
    "audio_bytes = bytes(bytearray(audio))\n",
    "n_frames = 4000\n",
    "thresh = 0\n",
    "conf = [] #store confidence of detected word\n",
    "word = [] #store words\n",
    "\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "rec.SetWords(True)\n",
    "\n",
    "#Initialize a time bar to see progress when analyzing\n",
    "file_size = len(audio)\n",
    "pbar = tqdm(total=file_size)\n",
    "\n",
    "#Run Speech Recognizer\n",
    "i = 0\n",
    "while True:\n",
    "    data = audio_bytes[n_frames*i:n_frames*(i+1)]\n",
    "    i += 1\n",
    "    pbar.update(len(data)/2) #updates progress bar\n",
    "\n",
    "    if data == b\"\": #if indexing is out of range, we get b\"\" as output\n",
    "        break\n",
    "\n",
    "    if rec.AcceptWaveform(data):\n",
    "        try: #if section does not contain a word, we get KeyError\n",
    "            instance = json.loads(rec.Result())\n",
    "            #if min. confidence, append detected words to lists\n",
    "            for entry in instance[\"result\"]:\n",
    "                if entry[\"conf\"] > thresh:\n",
    "                    conf.append(entry[\"conf\"])\n",
    "                    word.append(entry[\"word\"])\n",
    "\n",
    "        except KeyError: #catch error\n",
    "            continue\n",
    "\n",
    "#last detected word is not in rec.Result(), therefore...\n",
    "last_instance = json.loads(rec.FinalResult())\n",
    "for entry in last_instance[\"result\"]:\n",
    "    if entry[\"conf\"] > thresh:\n",
    "        conf.append(entry[\"conf\"])\n",
    "        word.append(entry[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get',\n",
       " 'the',\n",
       " 'soap',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bar',\n",
       " 'navigate',\n",
       " 'to',\n",
       " 'the',\n",
       " 'sink',\n",
       " 'and',\n",
       " 'bring',\n",
       " 'it',\n",
       " 'to',\n",
       " 'jacob']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Whisper (Open AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#wav_audio = \"C:/Users/Daydreamore/Desktop/Semester/Speech_to_Text/1/2_1.wav\"\n",
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "model = whisper.load_model(\"base\")\n",
    "audio = whisper.load_audio(\"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\") #\"C:/Users/Daydreamore/Desktop/record.mp3\"\n",
    "audio = whisper.pad_or_trim(audio) #pad/trim to fit 30 seconds\n",
    "#audio = audio.astype(dtype=\"float32\")\n",
    "result = model.transcribe(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Get the soap from the bar navigate to the sink and bring it to Jacob'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Speech to Text API ($)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio data can either be loaded from Google Cloud Storage or directly if the audio length is <1 minute and <10mb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get the soap from the bar navigate to the sink and bring it to Jacob\n"
     ]
    }
   ],
   "source": [
    "#pip install --upgrade google-cloud-speech\n",
    "#Documentation: https://cloud.google.com/speech-to-text/docs/speech-to-text-requests\n",
    "from google.cloud import speech\n",
    "client = speech.SpeechClient.from_service_account_file(\"google_key.json\")\n",
    "\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/7_7.wav\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    audio_data = f.read()\n",
    "\n",
    "audio_file = speech.RecognitionAudio(content = audio_data)\n",
    "\n",
    "#Note: for non-wav (or flac) files, an encoding parameter must be passed to config\n",
    "config = speech.RecognitionConfig(\n",
    "    sample_rate_hertz = 44100,\n",
    "    language_code = \"en-US\",\n",
    "    #model = default\n",
    ")\n",
    "\n",
    "response = client.recognize(\n",
    "    config = config,\n",
    "    audio = audio_file,\n",
    ")\n",
    "\n",
    "print(response.results[0].alternatives[0].transcript)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209955,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wave\n",
    "fs, audio_2 = wavfile.read(\"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\")\n",
    "audio_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundfile in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (0.1.97)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: pyaudio in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (0.2.13)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (from soundfile) (1.15.1)\n",
      "Requirement already satisfied: torch==1.13.0 in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (from torchaudio) (1.13.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (from torch==1.13.0->torchaudio) (4.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install soundfile sentencepiece torchaudio pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 158/158 [00:00<00:00, 39.4kB/s]\n",
      "Downloading: 100%|██████████| 291/291 [00:00<00:00, 41.6kB/s]\n",
      "Downloading: 100%|██████████| 162/162 [00:00<00:00, 81.3kB/s]\n",
      "Downloading: 100%|██████████| 85.0/85.0 [00:00<00:00, 42.7kB/s]\n",
      "Downloading: 100%|██████████| 1.57k/1.57k [00:00<00:00, 803kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/wav2vec2-large-960h-lv60-self\" # 1.18GB\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription(audio_path):\n",
    "  # load our wav file\n",
    "  speech, sr = torchaudio.load(audio_path)\n",
    "  speech = speech.squeeze()\n",
    "  # or using librosa\n",
    "  # speech, sr = librosa.load(audio_file, sr=16000)\n",
    "  # resample from whatever the audio sampling rate to 16000\n",
    "  resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "  speech = resampler(speech)\n",
    "  # tokenize our wav\n",
    "  input_values = processor(speech, return_tensors=\"pt\", sampling_rate=16000)[\"input_values\"]\n",
    "  # perform inference\n",
    "  logits = model(input_values)[\"logits\"]\n",
    "  # use argmax to get the predicted IDs\n",
    "  predicted_ids = torch.argmax(logits, dim=-1)\n",
    "  # decode the IDs to text\n",
    "  transcription = processor.decode(predicted_ids[0])\n",
    "  return transcription.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18G/1.18G [01:34<00:00, 13.4MB/s]\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'get the soap from the bar navigate to the sink and bring it to jacob'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "get_transcription(\"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:41:22) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d5ec8849436b7a03feff4b060e46c6b60d2401a18d2e1ee71f98cd3f572d4d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
