{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech_to_Text notebook to try out models and build a class that eventually automates testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import json\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import numpy as np\n",
    "import os\n",
    "#import scipy\n",
    "import seaborn as sns\n",
    "#from tqdm.notebook import tqdm\n",
    "#from vosk import Model, KaldiRecognizer\n",
    "\n",
    "\n",
    "#from importlib import reload\n",
    "#from scipy.io import wavfile\n",
    "#from IPython.display import Audio\n",
    "\n",
    "#Import the speech-to-text models\n",
    "from google_api import init_google, run_google\n",
    "from vosk_api import init_vosk, run_vosk\n",
    "from wav2vec2_api import init_wav2vec2, run_wav2vec2\n",
    "from whisper_api import init_whisper, run_whisper\n",
    "\n",
    "#Import metrics\n",
    "from metrics import rtf, wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_api import init_google, run_google\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_google()\n",
    "out, time = run_google(file_path, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper_api import init_whisper, run_whisper\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_whisper()\n",
    "out, time = run_whisper(file_path, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vosk_api import init_vosk, run_vosk\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_vosk()\n",
    "out, time = run_vosk(file_path, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\transformers\\feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('get the soap from the bar navigate to the sink and bring it to jacob',\n",
       " 1.696542501449585)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wav2vec2_api import init_wav2vec2, run_wav2vec2\n",
    "file_path = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/7/2_7.wav\"\n",
    "setup = init_wav2vec2()\n",
    "out, time = run_wav2vec2(file_path, setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load paths of recordings to pass to speech_to_text class and create dict with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "for file in natsorted(os.listdir(\"H:/Speech_to_Text/7\")):\n",
    "    paths.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sentences (partially generated with the General Purpose Service Robot Command Generator)\n",
    "tests = {1: \"pick up the bag\",\n",
    "         2: \"open the door\",\n",
    "         3: \"touch the desk\",\n",
    "         4: \"could you please hand the coke to lisa\",\n",
    "         5: \"put the spoon next to the bowl\",\n",
    "         6: \"place the tab inside the dishwasher\",\n",
    "         7: \"get the soap from the bar navigate to the sink and bring it to jacob\",\n",
    "         8: \"tell the time find the waving person in the living room, and escort him to the dining table\",\n",
    "         9: \"could you locate alexander in the bed guide him to the apartment, and follow emma who is at the tv stand\",\n",
    "         10: \"could you please take the left-most object from the bookcase to the shelf\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init class speech_to_text \n",
    "(When finished) handles the initialization and running of the different models, as well as comparison to ground truth and calculation of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class speech_to_text():\n",
    "\n",
    "    def __init__(self):\n",
    "        #Initialize models that will be compared\n",
    "        self.setup_whisper = init_whisper()\n",
    "        self.setup_google = init_google()\n",
    "        self.setup_vosk = init_vosk()\n",
    "        self.setup_wav2vec2 = init_wav2vec2()\n",
    "        print(\"All models initialized\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def load_data(self, file_directory, ground_truth):\n",
    "        \"\"\"\n",
    "        - file_directory: directory that contains audio files (.wav)\n",
    "        - ground truth: true text of the speech in the audio files\n",
    "        \"\"\"\n",
    "        self.file_directory = file_directory\n",
    "        self.ground_truth = ground_truth\n",
    "        self.file_paths = [file for file in natsorted(os.listdir(file_directory))]\n",
    "\n",
    "    def compare(self):\n",
    "        out_dict = {}\n",
    "        sum_dict = {\"Whisper\": {\"WER\": [], \"RTF\": []},\n",
    "                    \"Google\": {\"WER\": [], \"RTF\": []},\n",
    "                    \"Vosk\": {\"WER\": [], \"RTF\": []},\n",
    "                    \"Wav2vec2\": {\"WER\": [], \"RTF\": []}\n",
    "                    }\n",
    "        for ix, file in enumerate(self.file_paths):\n",
    "            file_name = \"{}/{}\".format(self.file_directory, file)\n",
    "\n",
    "            #Run models\n",
    "            out_whisper, time_whisper = run_whisper(file_name, self.setup_whisper)\n",
    "            out_google, time_google = run_google(file_name, self.setup_google)\n",
    "            out_vosk, time_vosk = run_vosk(file_name, self.setup_vosk)\n",
    "            out_wav2vec2, time_wav2vec2 = run_wav2vec2(file_name, self.setup_wav2vec2)\n",
    "\n",
    "            #Store metrics & model output in a nested dictionary\n",
    "            test_num = str(ix+1)\n",
    "            out_dict[test_num] = {\n",
    "                \"Whisper\": {\"WER\": wer(self.ground_truth, out_whisper), \n",
    "                            \"RTF\": rtf(time_whisper, file_name), \n",
    "                            \"Model Output\": out_whisper},\n",
    "                \"Google\": {\"WER\": wer(self.ground_truth, out_google), \n",
    "                           \"RTF\": rtf(time_google, file_name),\n",
    "                           \"Model Output\": out_google},\n",
    "                \"Vosk\": {\"WER\": wer(self.ground_truth, out_vosk),\n",
    "                         \"RTF\": rtf(time_vosk, file_name), \n",
    "                         \"Model Output\": out_vosk},\n",
    "                \"Wav2vec2\": {\"WER\": wer(self.ground_truth, out_wav2vec2), \n",
    "                             \"RTF\": rtf(time_wav2vec2, file_name),\n",
    "                             \"Model Output\": out_wav2vec2}\n",
    "                }\n",
    "            \n",
    "            #Keep track of the metrics for each model to later summarize\n",
    "            for model in sum_dict.keys():\n",
    "                for metric in sum_dict[model].keys():\n",
    "                    sum_dict[model][metric].append(out_dict[test_num][model][metric])\n",
    "        \n",
    "        for model in sum_dict.keys():\n",
    "                for metric in sum_dict[model].keys():\n",
    "                    #Calculate mean for each metric for each model\n",
    "                    sum_dict[model][metric] = round(sum(sum_dict[model][metric])/len(sum_dict[model][metric]),2)\n",
    "        \n",
    "        #Save output (better than returning when running all models with all audios in a loop)\n",
    "        np.save(\"comparisons/{}_detailed.npy\".format(str(self.counter)),out_dict)\n",
    "        np.save(\"comparisons/{}_summarized.npy\".format(str(self.counter)),sum_dict)\n",
    "        self.counter += 1\n",
    "        #return out_dict, sum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = np.load(\"first_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models initialized\n"
     ]
    }
   ],
   "source": [
    "test_class = speech_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Daydreamore\\anaconda3\\envs\\svenpy\\lib\\site-packages\\transformers\\feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "#out = test_class.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in np.arange(1,11,1):\n",
    "    dir = \"C:/Users/Daydreamore/Desktop/Semester/speech_recognition/recordings/{}\".format(str(i))\n",
    "    test_class.load_data(file_directory=dir, ground_truth=tests[i])\n",
    "    test_class.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svenpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d5ec8849436b7a03feff4b060e46c6b60d2401a18d2e1ee71f98cd3f572d4d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
